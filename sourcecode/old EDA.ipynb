{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.metrics import accuracy_score  \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "df=pd.read_csv(r\"\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check dublicates\n",
    "duplicated_data=df.duplicated()\n",
    "print(duplicated_data)\n",
    "\n",
    "duplicates_of_data = df[df.duplicated(keep=False)]\n",
    "print(duplicates_of_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check null values\n",
    "df.isna()\n",
    "rows_with_nan = df[df.isna().any(axis=1)]\n",
    "print(rows_with_nan)\n",
    "\n",
    "print(df.isnull().sum())\n",
    "missing_percentage = df.isnull().mean() * 100\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if there is null values\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "\n",
    "# # تحويل النصوص إلى قيم رقمية باستخدام LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# df['Alcohol_Consumption'] = label_encoder.fit_transform(df['Alcohol_Consumption'])\n",
    "\n",
    "# # الآن يمكنك تطبيق IterativeImputer\n",
    "# imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "# df['Alcohol_Consumption'] = imputer.fit_transform(df[['Alcohol_Consumption']])\n",
    "\n",
    "# # عرض البيانات بعد المعالجة\n",
    "# print(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # إنشاء نسخة احتياطية من القيم النصية\n",
    "# df['Alcohol_Consumption_Original'] = df['Alcohol_Consumption']\n",
    "\n",
    "# # تحويل القيم النصية إلى أرقام\n",
    "# label_encoder = LabelEncoder()\n",
    "# df['Alcohol_Consumption'] = label_encoder.fit_transform(df['Alcohol_Consumption'])\n",
    "\n",
    "# # إعداد التعمير باستخدام نموذج تصنيف\n",
    "# imputer = IterativeImputer(estimator=RandomForestClassifier(), max_iter=10, random_state=0)\n",
    "# df['Alcohol_Consumption'] = imputer.fit_transform(df[['Alcohol_Consumption']]).astype(int)\n",
    "\n",
    "# # إعادة القيم إلى النصوص الأصلية بعد التعمير\n",
    "# df['Alcohol_Consumption'] = label_encoder.inverse_transform(df['Alcohol_Consumption'])\n",
    "\n",
    "# # عرض البيانات بعد المعالجة\n",
    "# print(df[['Alcohol_Consumption_Original', 'Alcohol_Consumption']])\n",
    "\n",
    "\n",
    "\n",
    "# print(df.isnull().sum())  # لمعرفة عدد القيم المفقودة بعد التعمير\n",
    "# print(df[df['Alcohol_Consumption'].isnull()])  # عرض الصفوف التي لم يتم تعميرها\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# # التحقق من أي قيم مفقودة متبقية\n",
    "# simple_imputer = SimpleImputer(strategy='most_frequent')\n",
    "# df[['Alcohol_Consumption']] = simple_imputer.fit_transform(df[['Alcohol_Consumption']])\n",
    "# df.dropna(subset=['Alcohol_Consumption'], inplace=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Alcohol_Consumption'].fillna(df['Alcohol_Consumption'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# # حساب المتوسط فقط للأعمدة العددية\n",
    "# df.fillna(df.mode().iloc[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert Blood Pressure to Systolic and Diastolic\n",
    "# df[['Systolic', 'Diastolic']] = df['Blood Pressure'].str.split('/', expand=True).astype(int)\n",
    "# df.drop('Blood Pressure', axis=1, inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # تقسيم القيم التي تحتوي على \"-\"\n",
    "# df[['age start', 'age end']] = df['Age_Category'].str.split('-', expand=True)\n",
    "\n",
    "# # التعامل مع القيم التي تحتوي على \"+\"\n",
    "# df['age end'] = df['age end'].fillna(df['age start'])\n",
    "\n",
    "\n",
    "# # حذف العمود الأصلي\n",
    "# df.drop('Age_Category', axis=1, inplace=True)\n",
    "\n",
    "# # عرض النتيجة\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features don't affect on risk\n",
    "# Drop Unneeded columns\n",
    "# df.drop(['Sedentary Hours Per Day','Income','Country','Continent','Hemisphere','Sedentary Hours Per Day'],axis=1,inplace=True)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put target column data in last column\n",
    "new = df['HeartDisease']\n",
    "df.drop(columns=['HeartDisease'], axis=1, inplace=True)\n",
    "df['HeartDisease'] = new\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'Age'\n",
    "\n",
    "mean_value = df[column_name].mean()\n",
    "\n",
    "df[column_name] = df[column_name].apply(lambda x: mean_value if x >= 100 else x)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#know datatype of data\n",
    "numeric_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(\"Numeric Columns:\", numeric_columns)\n",
    "print(\"Categorical Columns:\", categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify outliers\n",
    "outliers_summary = {}\n",
    "for col in numeric_columns:\n",
    "    Q1 = df[col].quantile(0.25)  \n",
    "    Q3 = df[col].quantile(0.75)  \n",
    "    IQR = Q3 - Q1                  \n",
    "   \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    outliers_count = outliers.shape[0]\n",
    "    outliers_summary[col] = outliers_count\n",
    "\n",
    "for col, count in outliers_summary.items():\n",
    "    print(f\"Column: {col}, Outliers: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# حساب الـ IQR\n",
    "Q1 = df['Fruit_Consumption'].quantile(0.25)\n",
    "Q3 = df['Fruit_Consumption'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# تحديد الحدود العليا والدنيا باستخدام الـ IQR\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# استبدال القيم الشاذة بالوسيط\n",
    "median_value = df['Fruit_Consumption'].median()\n",
    "\n",
    "df['Fruit_Consumption'] = np.where(df['Fruit_Consumption'] < lower_bound, median_value, df['Fruit_Consumption'])\n",
    "df['Fruit_Consumption'] = np.where(df['Fruit_Consumption'] > upper_bound, median_value, df['Fruit_Consumption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of sex on Heart Attack Risk:\n",
    "pie_fig = px.pie(\n",
    "    df, names=\"Gender\", values=\"Alzheimer_risk\",\n",
    "    title=\"Effect of sex on Heart Attack Risk\"\n",
    ")\n",
    "pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "pie_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Diabetes on Heart Attack Risk:\n",
    "# pie_fig = px.pie(\n",
    "#     df, names=\"Diabetes\", values=\"Heart Attack Risk\",\n",
    "#     title=\"Effect of Diabetes on Heart Attack Risk\"\n",
    "# )\n",
    "# pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "# pie_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Family History on Heart Attack Risk:\n",
    "# pie_fig = px.pie(\n",
    "#     df, names=\"Family History\", values=\"Heart Attack Risk\",\n",
    "#     title=\"Effect of Family History on Heart Attack Risk\"\n",
    "# )\n",
    "# pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "# pie_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Smoking on Heart Attack Risk:\n",
    "# pie_fig = px.pie(\n",
    "#     df, names=\"Smoking\", values=\"Heart Attack Risk\",\n",
    "#     title=\"Effect of Smoking on Heart Attack Risk\"\n",
    "# )\n",
    "# pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "# pie_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Obesity on Heart Attack Risk:\n",
    "# pie_fig = px.pie(\n",
    "#     df, names=\"Obesity\", values=\"Heart Attack Risk\",\n",
    "#     title=\"Effect of Obesity on Heart Attack Risk\"\n",
    "# )\n",
    "# pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "# pie_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Alcohol Consumption on Heart Attack Risk:\n",
    "# pie_fig = px.pie(\n",
    "#     df, names=\"Alcohol Consumption\", values=\"Heart Attack Risk\",\n",
    "#     title=\"Effect of Alcohol Consumption on Heart Attack Risk\"\n",
    "# )\n",
    "# pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "# pie_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Previous Heart Problems on Heart Attack Risk:\n",
    "# pie_fig = px.pie(\n",
    "#     df, names=\"Previous Heart Problems\", values=\"Heart Attack Risk\",\n",
    "#     title=\"Effect of Previous Heart Problems on Heart Attack Risk\"\n",
    "# )\n",
    "# pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "# pie_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Medication Use on Heart Attack Risk:\n",
    "# pie_fig = px.pie(\n",
    "#     df, names=\"Medication Use\", values=\"Heart Attack Risk\",\n",
    "#     title=\"Effect of Medication Use on Heart Attack Risk\"\n",
    "# )\n",
    "# pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "# pie_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Continent on Heart Attack Risk:\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# sns.barplot(x=df['Continent'], y=df['Heart Attack Risk'])\n",
    "# plt.title(\"Effect of Continent on Heart Attack Risk\")\n",
    "# plt.xlabel(\"Continent\")\n",
    "# plt.ylabel(\"Heart Attack Probability\")\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Encode categorical variables if necessary\n",
    "# le_Sex = LabelEncoder()\n",
    "# le_Diet = LabelEncoder()\n",
    "# le_Country= LabelEncoder()\n",
    "# le_Continent= LabelEncoder()\n",
    "# le_Hemisphere= LabelEncoder()\n",
    "# df['Sex'] = le_Sex.fit_transform(df['Sex']) \n",
    "# df['Diet'] =le_Diet.fit_transform(df['Diet'])\n",
    "# df['Country'] =le_Diet.fit_transform(df['Country'])\n",
    "# df['Continent'] =le_Diet.fit_transform(df['Continent'])\n",
    "# df['Hemisphere'] =le_Diet.fit_transform(df['Hemisphere'])\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # تحديد الأعمدة الرقمية باستثناء target column\n",
    "# numeric_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# # استبعاد target column من عملية التحجيم\n",
    "# target_column = 'Heart_Disease'  # استبدل باسم العمود المستهدف\n",
    "# numeric_columns.remove(target_column)\n",
    "\n",
    "# # تطبيق التحجيم على الأعمدة الرقمية فقط\n",
    "# scaler = StandardScaler()\n",
    "# df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# # عرض أول 5 صفوف من الأعمدة التي تم تحجيمها\n",
    "# print(df[numeric_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تحديد الأعمدة\n",
    "columns_to_convert = [\"Smoking\", \"Medication\", \"Obesity\", \"Heart_Attack\", \"Diabetes\", \"Family_History\", \"Angina\", \"Heart_Disease_History\"]\n",
    "\n",
    "# تحويل قيم True/False إلى 1/0 فقط في الأعمدة المحددة\n",
    "df[columns_to_convert] = df[columns_to_convert].astype(int)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #data scalling\n",
    "# numeric_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "# scaler = StandardScaler()\n",
    "# df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "# print(df[numeric_columns].head())\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc=LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    df[col] = enc.fit_transform(df[col])\n",
    "\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# إنشاء دالة للتحقق من القيم في العمود\n",
    "def apply_scaling_if_needed(df, column_name):\n",
    "    unique_values = df[column_name].unique()\n",
    "    \n",
    "    # إذا كان العمود يحتوي فقط على 0 و 1\n",
    "    if set(unique_values).issubset({0, 1}):\n",
    "        print(f\"لا حاجة لتطبيق Scaling على العمود {column_name} لأنه يحتوي فقط على 0 و 1\")\n",
    "    else:\n",
    "        # إذا كان العمود يحتوي على قيم أخرى (مثل 2 أو أكثر)\n",
    "        scaler = StandardScaler()\n",
    "        df[column_name] = scaler.fit_transform(df[[column_name]])\n",
    "        print(f\"تم تطبيق Scaling على العمود {column_name}\")\n",
    "    \n",
    "# تطبيق الدالة على الأعمدة\n",
    "for column in df.columns:\n",
    "    apply_scaling_if_needed(df, column)\n",
    "\n",
    "# عرض البيانات بعد تطبيق التحجيم\n",
    "print(\"\\nالبيانات بعد التحجيم:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove features don't affect on risk\n",
    "#Drop Unneeded columns\n",
    "# df.drop(['Occupation','Income_Level','Marital_Status','Education_Level','Urban_Rural','Region'],axis=1,inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency, f_oneway\n",
    "\n",
    "\n",
    "# تحويل الأعمدة الفئوية إلى نوع category\n",
    "categorical_features = ['Country', 'Gender', 'Education Level', 'Smoking Status', 'Alcohol Consumption',\n",
    "                        'Diabetes', 'Hypertension', 'Family History of Alzheimer’s', 'Marital Status',\n",
    "                        'Genetic Risk Factor (APOE-ε4 allele)', 'Urban vs Rural Living', 'Employment Status']\n",
    "\n",
    "df[categorical_features] = df[categorical_features].astype('category')\n",
    "\n",
    "# حساب مصفوفة الارتباط للميزات الرقمية\n",
    "numerical_features = ['Age', 'BMI', 'Physical Activity Level', 'Cholesterol Level', 'Cognitive Test Score',\n",
    "                      'Depression Level', 'Sleep Quality', 'Dietary Habits', 'Air Pollution Exposure',\n",
    "                      'Social Engagement Level', 'Income Level', 'Stress Levels']\n",
    "\n",
    "correlation_matrix = df[numerical_features + ['Alzheimer risk']].corr()\n",
    "\n",
    "# رسم خريطة الحرارة Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# اختبار الأهمية الإحصائية للميزات الفئوية باستخدام Chi-Square Test\n",
    "chi2_results = {}\n",
    "for col in categorical_features:\n",
    "    contingency_table = pd.crosstab(df[col], df['Alzheimer risk'])\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "    chi2_results[col] = p\n",
    "\n",
    "# طباعة القيم p-value (كلما كانت أصغر من 0.05، كانت الميزة مؤثرة)\n",
    "print(\"Chi-Square Test p-values for Categorical Features:\")\n",
    "print(pd.Series(chi2_results).sort_values())\n",
    "\n",
    "# اختبار الأهمية الإحصائية للميزات العددية باستخدام ANOVA\n",
    "anova_results = {}\n",
    "for col in numerical_features:\n",
    "    groups = [df[df['Alzheimer risk'] == val][col].dropna() for val in df['Alzheimer risk'].unique()]\n",
    "    f_stat, p = f_oneway(*groups)\n",
    "    anova_results[col] = p\n",
    "\n",
    "# طباعة القيم p-value\n",
    "print(\"ANOVA Test p-values for Numerical Features:\")\n",
    "print(pd.Series(anova_results).sort_values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('HeartDisease', axis=1) \n",
    "y = df['HeartDisease']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(\"Random Forest Mean Squared Error:\", mean_squared_error(y_test, y_pred_rf))\n",
    "print(r2_score(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from xgboost import XGBClassifier\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # تقسيم البيانات\n",
    "# m = df.drop('Heart_Attack', axis=1)\n",
    "# n = df['Heart_Attack']\n",
    "\n",
    "# # تقسيم البيانات إلى تدريب واختبار\n",
    "# Xtrain, Xtest, Ytrain, Ytest = train_test_split(m, n, test_size=0.30, random_state=1)\n",
    "\n",
    "# # تطبيق SMOTE لزيادة بيانات الفئة الأقل\n",
    "# sm = SMOTE(random_state=1)\n",
    "# Xtrain_res, Ytrain_res = sm.fit_resample(Xtrain, Ytrain)\n",
    "\n",
    "# # 1️⃣ استخدام نموذج RandomForest مع تعديل المعايير\n",
    "# rf = RandomForestClassifier(class_weight='balanced', random_state=1, n_estimators=100, max_depth=10)\n",
    "# rf.fit(Xtrain_res, Ytrain_res)\n",
    "\n",
    "# # التنبؤ على بيانات الاختبار\n",
    "# rf_predict = rf.predict(Xtest)\n",
    "\n",
    "# # حساب الدقة\n",
    "# rf_train_acc = rf.score(Xtrain_res, Ytrain_res)\n",
    "# rf_test_acc = accuracy_score(Ytest, rf_predict)\n",
    "\n",
    "# # طباعة نتائج RandomForest\n",
    "# print(\"Random Forest Training Accuracy:\", rf_train_acc)\n",
    "# print(\"Random Forest Test Accuracy:\", rf_test_acc)\n",
    "# print(\"Confusion Matrix for Random Forest:\\n\", confusion_matrix(Ytest, rf_predict))\n",
    "\n",
    "# # 2️⃣ استخدام XGBoost\n",
    "# xgb = XGBClassifier(scale_pos_weight=3, random_state=1)\n",
    "# xgb.fit(Xtrain_res, Ytrain_res)\n",
    "\n",
    "# # التنبؤ على بيانات الاختبار\n",
    "# xgb_predict = xgb.predict(Xtest)\n",
    "\n",
    "# # حساب الدقة\n",
    "# xgb_test_acc = accuracy_score(Ytest, xgb_predict)\n",
    "\n",
    "# # طباعة نتائج XGBoost\n",
    "# print(\"XGBoost Test Accuracy:\", xgb_test_acc)\n",
    "# print(\"Confusion Matrix for XGBoost:\\n\", confusion_matrix(Ytest, xgb_predict))\n",
    "\n",
    "# # 3️⃣ تعديل العتبة (Threshold) وتحليل النتائج\n",
    "# rf_prob = rf.predict_proba(Xtest)[:, 1]  # احتمالات الفئة الإيجابية\n",
    "# rf_predict_new = (rf_prob >= 0.3).astype(int)  # عتبة 30%\n",
    "\n",
    "# # تقييم النموذج مع العتبة المعدلة\n",
    "# print(\"Confusion Matrix with adjusted threshold for Random Forest:\")\n",
    "# print(confusion_matrix(Ytest, rf_predict_new))\n",
    "\n",
    "# # حساب الدقة مع العتبة المعدلة\n",
    "# rf_test_acc_new = accuracy_score(Ytest, rf_predict_new)\n",
    "# print(\"Random Forest Test Accuracy with adjusted threshold:\", rf_test_acc_new)\n",
    "\n",
    "# # 4️⃣ Cross-validation باستخدام RandomForest و XGBoost\n",
    "# rf_cv_scores = cross_val_score(rf, Xtrain_res, Ytrain_res, cv=5, scoring='accuracy')\n",
    "# xgb_cv_scores = cross_val_score(xgb, Xtrain_res, Ytrain_res, cv=5, scoring='accuracy')\n",
    "\n",
    "# print(\"Random Forest Cross-validation scores:\", rf_cv_scores)\n",
    "# print(\"XGBoost Cross-validation scores:\", xgb_cv_scores)\n",
    "\n",
    "# # 5️⃣ رسم النتائج\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# # رسم دقة التدريب والاختبار لكل نموذج\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.bar(['Random Forest', 'XGBoost'], [rf_test_acc, xgb_test_acc])\n",
    "# plt.ylabel('Test Accuracy')\n",
    "# plt.title('Model Comparison')\n",
    "\n",
    "# # رسم نتائج cross-validation\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.bar(['Random Forest', 'XGBoost'], [rf_cv_scores.mean(), xgb_cv_scores.mean()])\n",
    "# plt.ylabel('Cross-validation Accuracy')\n",
    "# plt.title('Cross-validation Comparison')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # تطبيق SMOTE لزيادة بيانات الفئة الأقل\n",
    "# sm = SMOTE(random_state=1)\n",
    "# Xtrain_res, Ytrain_res = sm.fit_resample(Xtrain, Ytrain)\n",
    "\n",
    "# # تدريب نموذج XGBoost مع ضبط scale_pos_weight\n",
    "# xgb = XGBClassifier(scale_pos_weight=3, random_state=1)\n",
    "# xgb.fit(Xtrain_res, Ytrain_res)\n",
    "\n",
    "# # التنبؤ على بيانات الاختبار\n",
    "# xgb_predict = xgb.predict(Xtest)\n",
    "\n",
    "# # حساب الدقة\n",
    "# xgb_test_acc = accuracy_score(Ytest, xgb_predict)\n",
    "\n",
    "# # طباعة النتائج\n",
    "# print(\"XGBoost Test Accuracy:\", xgb_test_acc)\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(Ytest, xgb_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first model \"LogisticRegression\" without any improvement    veryyyyy gooooood\n",
    "\n",
    "logr=LogisticRegression()\n",
    "logr.fit(Xtrain,Ytrain)\n",
    "logR_predict=logr.predict(Xtest)\n",
    "Tr_score=logr.score(Xtrain,Ytrain)\n",
    "Tst_score=logr.score(Xtest,Ytest)\n",
    "\n",
    "print(\"Complete LogisticRegression Training !\",Tr_score)\n",
    "print(\"Complete LogisticRegression Test!\",Tst_score)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(Ytest, logR_predict))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(Ytest, logR_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #first model \"LogisticRegression\" with Smote and Class Weigh\n",
    "\n",
    "\n",
    "# # smote to increase number of smaller data\n",
    "# sm = SMOTE(random_state=1)\n",
    "# Xtrain_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# model= LogisticRegression(class_weight='balanced', random_state=1)\n",
    "# model.fit(Xtrain_res, y_train_res)\n",
    "\n",
    "# # Logistic Regression\n",
    "# model = LogisticRegression()\n",
    "# model.fit(Xtrain_res, y_train_res)\n",
    "\n",
    "# # logisticRegression\n",
    "# logR_predict = model.predict(X_test)\n",
    "\n",
    "# # accuracy\n",
    "# Tr_score = model.score(Xtrain_res, y_train_res)\n",
    "# Tst_score = model.score(X_test, y_test)\n",
    "\n",
    "# print(\"Complete LogisticRegression Training Accuracy:\", Tr_score)\n",
    "# print(\"Complete LogisticRegression Test Accuracy:\", Tst_score)\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, logR_predict))\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(y_test, logR_predict))\n",
    "\n",
    "# plt.figure(figsize=(5, 4))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Diabetes\", \"Diabetes\"], yticklabels=[\"No Diabetes\", \"Diabetes\"])\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Actual\")\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #first model \"LogisticRegression\" with Smote & Class Weigh & threshold & solver\n",
    "# Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=0.30, random_state=1)\n",
    "\n",
    "\n",
    "# # sm = SMOTE(random_state=1)\n",
    "# # Xtrain_res, Ytrain_res = sm.fit_resample(Xtrain, Ytrain)\n",
    "\n",
    "\n",
    "# logR = LogisticRegression(class_weight=\"balanced\", solver='saga', random_state=1)\n",
    "# logR.fit(Xtrain_res, Ytrain_res)\n",
    "\n",
    "# y_probs = logR.predict_proba(Xtest)[:, 1]  \n",
    "\n",
    "\n",
    "# threshold = 0.5\n",
    "# logR_predict = (y_probs >= threshold).astype(int)\n",
    "\n",
    "# # حساب الدقة والتقارير\n",
    "# train_accuracy = accuracy_score(Ytrain_res, logR.predict(Xtrain_res))\n",
    "# test_accuracy = accuracy_score(Ytest, logR_predict)\n",
    "# roc_auc = roc_auc_score(Ytest, y_probs)\n",
    "\n",
    "# print(\"Complete LogisticRegression Training Accuracy:\", train_accuracy)\n",
    "# print(\"Complete LogisticRegression Test Accuracy:\", test_accuracy)\n",
    "# print(\"ROC-AUC Score:\", roc_auc)\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(Ytest, logR_predict))\n",
    "# print(\"\\nConfusion Matrix:\\n\", confusion_matrix(Ytest, logR_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # تقسيم البيانات\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # تطبيق SMOTE\n",
    "# smote = SMOTE(sampling_strategy=0.8, random_state=42)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # حساب scale_pos_weight تلقائيًا\n",
    "# scale_pos_weight = len(y_train_resampled[y_train_resampled == 0]) / len(y_train_resampled[y_train_resampled == 1])\n",
    "\n",
    "# # مقياس البيانات\n",
    "# scaler = StandardScaler()\n",
    "# X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # إنشاء النموذج بدون Early Stopping\n",
    "# xgb = XGBClassifier(\n",
    "#     scale_pos_weight=scale_pos_weight,  \n",
    "#     random_state=42, \n",
    "#     max_depth=4,         \n",
    "#     learning_rate=0.05,  \n",
    "#     n_estimators=500,  \n",
    "#     subsample=0.8,      \n",
    "#     colsample_bytree=0.7,\n",
    "#     reg_alpha=0.1,      \n",
    "#     reg_lambda=1.0,\n",
    "#     eval_metric=\"logloss\"\n",
    "# )\n",
    "\n",
    "# # ✅ تدريب النموذج بدون early_stopping_rounds\n",
    "# xgb.fit(X_train_resampled, y_train_resampled, eval_set=[(X_test, y_test)], verbose=True)\n",
    "\n",
    "# # التنبؤ بالاحتمالات\n",
    "# y_probs = xgb.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "# # ضبط العتبة\n",
    "# threshold = 0.6  \n",
    "# y_pred_xgb = (y_probs >= threshold).astype(int)\n",
    "\n",
    "# # التقييم\n",
    "# test_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "# roc_auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "# print(\"🔹 XGBoost Accuracy:\", test_accuracy)\n",
    "# print(\"🔹 ROC-AUC Score:\", roc_auc)\n",
    "# print(\"🔹 Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
    "# print(\"🔹 Classification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #جامدههههههه\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from imblearn.combine import SMOTEENN\n",
    "# from lightgbm import early_stopping\n",
    "\n",
    "# smote_enn = SMOTEENN(random_state=42)\n",
    "# X_train_res, y_train_res = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# # # موازنة البيانات باستخدام SMOTE\n",
    "# # smote = SMOTE(random_state=42)\n",
    "# # X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # تدريب LightGBM مع تحسين المعلمات لتجنب Overfitting\n",
    "# lgbm = LGBMClassifier(\n",
    "#     num_leaves=6,              # تقليل تعقيد النموذج\n",
    "#     max_depth=1,                # تقليل العمق\n",
    "#     learning_rate=0.03,         # معدل تعلم أصغر لتحسين التعميم\n",
    "#     n_estimators=200,           # تقليل عدد الأشجار\n",
    "#     reg_lambda=10.0,             # L2 Regularization\n",
    "#     reg_alpha=6,  \n",
    "#     min_child_samples=50,             # L1 Regularization\n",
    "#     bagging_fraction=0.6,       # تقليل الاعتماد على بيانات محددة\n",
    "#     feature_fraction=0.6, \n",
    "#     bagging_freq=5,     # تقليل الاعتماد على ميزات معينة\n",
    "#     random_state=42\n",
    "# )\n",
    "# lgbm.fit(\n",
    "#     X_train_res, \n",
    "#     y_train_res,\n",
    "#     eval_set=[(X_test, y_test)],  \n",
    "#     eval_metric=\"logloss\",  \n",
    "#     callbacks=[early_stopping(stopping_rounds=20, verbose=True)]  \n",
    "    \n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# cv_scores = cross_val_score(lgbm, X_train_res, y_train_res, cv=5, scoring=\"accuracy\")\n",
    "# y_pred = lgbm.predict(X_test)\n",
    "# train_accuracy = accuracy_score(y_train_res, lgbm.predict(X_train_res))\n",
    "# test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "# print(f\"Mean CV Accuracy: {cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# حساب منحنى التعلم\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    lgbm, X_train_res, y_train_res, cv=5, scoring=\"accuracy\",\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "# حساب المتوسط والانحراف المعياري للدقة\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# رسم منحنى التعلم\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, 'b-', marker='o', label=\"Training Accuracy\")\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"blue\", alpha=0.2)\n",
    "\n",
    "plt.plot(train_sizes, val_mean, 'r-', marker='s', label=\"Validation Accuracy\")\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color=\"red\", alpha=0.2)\n",
    "\n",
    "plt.title(\"Learning Curve (Detecting Overfitting)\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6= lgb.LGBMClassifier()\n",
    "\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model6, X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 10), scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_sizes, train_mean, label=\"Training Accuracy\", color=\"blue\")\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color=\"blue\")\n",
    "\n",
    "plt.plot(train_sizes, test_mean, label=\"Validation Accuracy\", color=\"red\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2, color=\"red\")\n",
    "\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve (Detecting Overfitting)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # تعديل معلمات البحث لتحسين أداء LightGBM\n",
    "# param_grid = {\n",
    "    \n",
    "#     'num_leaves': [50, 100],  \n",
    "#     'max_depth': [10, 15],  \n",
    "#     'learning_rate': [0.05],  \n",
    "#     'n_estimators': [200, 300]  \n",
    "\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, scoring='accuracy', cv=5, verbose=2)\n",
    "# grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# # الحصول على أفضل معلمات\n",
    "# best_params = grid_search.best_params_\n",
    "# print(\"أفضل معلمات:\", best_params)\n",
    "\n",
    "# # تدريب النموذج باستخدام أفضل المعلمات\n",
    "# best_lgbm = grid_search.best_estimator_\n",
    "# y_pred_best_lgbm = best_lgbm.predict(X_test)\n",
    "\n",
    "# # تقييم النموذج\n",
    "# print(\"أفضل نموذج LightGBM Classification Report:\\n\", classification_report(y_test, y_pred_best_lgbm))\n",
    "# print(\"أفضل نموذج LightGBM Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_best_lgbm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import lightgbm as lgb\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# # 🔹 تحميل البيانات\n",
    "# m = df.drop('diabetes', axis=1)\n",
    "# n = df['diabetes']\n",
    "\n",
    "# # 🔹 تقسيم البيانات إلى تدريب واختبار\n",
    "# Xtrain, Xtest, Ytrain, Ytest = train_test_split(m, n, test_size=0.30, random_state=1)\n",
    "\n",
    "# # 🔹 تطبيق SMOTE لموازنة الفئات\n",
    "# sm = SMOTE(random_state=1)\n",
    "# Xtrain_res, Ytrain_res = sm.fit_resample(Xtrain, Ytrain)\n",
    "\n",
    "# # 🔹 ضبط scale_pos_weight لحل مشكلة عدم التوازن\n",
    "# scale_pos_weight = len(Ytrain[Ytrain == 0]) / len(Ytrain[Ytrain == 1])\n",
    "\n",
    "# # 🔹 إنشاء نموذج LightGBM مع المعلمات المحسنة\n",
    "# lgb_model = lgb.LGBMClassifier(\n",
    "#     scale_pos_weight=scale_pos_weight,  # موازنة الفئات\n",
    "#     random_state=1,\n",
    "#     n_estimators=150, \n",
    "#     reg_alpha=20, \n",
    "#     reg_lambda=10, # عدد الأشجار\n",
    "#     max_depth=3,  # تقليل التعقيد\n",
    "#     learning_rate=0.05 ,\n",
    "#     feature_fraction=0.8, \n",
    "#     bagging_fraction=0.8, \n",
    "#     bagging_freq=5# معدل التعلم\n",
    "# )\n",
    "\n",
    "# # 🔹 تدريب النموذج\n",
    "# lgb_model.fit(Xtrain_res, Ytrain_res)\n",
    "\n",
    "# # 🔹 التنبؤ باحتمالات الفئة 1\n",
    "# y_probs = lgb_model.predict_proba(Xtest)[:, 1]\n",
    "\n",
    "# # 🔹 تحديد العتبة المثلى باستخدام AUC-ROC\n",
    "# fpr, tpr, thresholds = roc_curve(Ytest, y_probs)\n",
    "# optimal_idx = np.argmax(tpr - fpr)  # إيجاد أفضل نقطة توازن\n",
    "# optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "# print(f\"🔹 Optimal Threshold Found: {optimal_threshold:.3f}\")\n",
    "\n",
    "# # 🔹 ضبط التنبؤات باستخدام العتبة المثلى\n",
    "# y_preds_adjusted = (y_probs >= optimal_threshold).astype(int)\n",
    "\n",
    "# # 🔹 حساب مقاييس التقييم\n",
    "# accuracy = accuracy_score(Ytest, y_preds_adjusted)\n",
    "# precision = precision_score(Ytest, y_preds_adjusted)\n",
    "# recall = recall_score(Ytest, y_preds_adjusted)\n",
    "# f1 = f1_score(Ytest, y_preds_adjusted)\n",
    "\n",
    "# # 🔹 طباعة النتائج\n",
    "\n",
    "# train_accuracy = accuracy_score(Ytrain_res, lgb_model.predict(Xtrain_res))\n",
    "\n",
    "# # حساب دقة الاختبار\n",
    "# test_accuracy = accuracy_score(Ytest, lgb_model.predict(Xtest))\n",
    "\n",
    "# print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# print(\"\\n🔹 LightGBM Performance with Optimized Threshold 🔹\")\n",
    "# print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"F1-Score: {f1:.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(Ytest, y_preds_adjusted))\n",
    "# print(\"أفضل نموذج LightGBM Classification Report:\\n\", classification_report(Ytest,y_preds_adjusted))\n",
    "\n",
    "# # 🔹 حساب Cross-validation\n",
    "# lgb_cv_scores = cross_val_score(lgb_model, Xtrain_res, Ytrain_res, cv=5, scoring='accuracy')\n",
    "# print(\"\\n🔹 LightGBM Cross-validation scores:\", lgb_cv_scores)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
